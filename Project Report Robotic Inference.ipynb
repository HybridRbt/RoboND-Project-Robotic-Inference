{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Robotic Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract: \n",
    "\n",
    "This is a report for Project: Robotic Inference. This project contains two parts: the first part is a practising of the NVIDIA DIGITS workflow, whose purpose is to gain familiarity with the workflow, and as a preparation for the second part of the project. A data set was provided and a network model was trained on that data set. The model was evaluated and has achieved an inference time of xx ms, and has an accuracy of XXX, which satisfied the requirement. For the second part of the project, an idea of training a classification network to classify cats is proposed. A customized data set consists of xxx images of cats was collected, and a network model was trained on that data set. The model was evaluated and has achieved an inference time of xx ms, and has an accuracy of XXX, which satisfied the requirement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Practicing with the DIGITS Workflow\n",
    "\n",
    "### Introduction: \n",
    "\n",
    "This is a practice of the NVIDIA DIGITS workflow. A detailed guidance is provided in the course along with the data set.\n",
    "\n",
    "### Data Acquisition\n",
    "* Data set is provided on DIGITS server\n",
    "\n",
    "* Create data set on DIGITS as instructed. Keep everything as default, and name it p1_data.\n",
    "   ![createDataSetP1](imgs/createDataSetP1.png)\n",
    "   \n",
    "* Observe the data set\n",
    "\n",
    "It can be seen that this data set has 3 categories: bottle, candy box, and nothing. After the split for training set and validation set, the numbers of images in each set are:\n",
    "   1. training set:\n",
    "     * bottle: 3426\n",
    "     * nothing: 2273\n",
    "     * candy box: 1871\n",
    "   2. validation set:\n",
    "     * bottle: 1142\n",
    "     * nothing: 758\n",
    "     * candy box: 624\n",
    "    \n",
    "So in total this set has 10094 images, and they are all color images sized 256 x 256.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background / Formulation:\n",
    "\n",
    "* Choose the network\n",
    "\n",
    "Based on the observation, googLeNet is chosen as the target network, and for a quick start, the epoch number is set to 10. Eeverything else was kept default. The model was named googLeNet_1, in which the 1 indicates first run for this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First run\n",
    "\n",
    "* Epoch is set to 10\n",
    "![googLeNet_model_1](imgs\\googleNetModel1.png)\n",
    "\n",
    "A interesting observation: after 1 epoch, the accuracy reaches 99%, after 3 epochs it reaches 100%, which seems too good to be true. After examining the data set more carefully, several reasons can be draw as to why the accuracy was so high:    \n",
    "  1. A comparatively large data set. For a data set that contains only 3 categories, it has over 10000 images, and this helps the network to learn more about the objects; <br><br>\n",
    "  2. All the imges have a unified background. Being captured in the same environment really helps the network to learn better. <br><br>\n",
    "  3. Simple objects. The bottle and candy box are all relatively simple, and each have a regular shape. <br><br>\n",
    "    \n",
    "#### Evaluation\n",
    "\n",
    "Here is the evalution for the network after first training:\n",
    "\n",
    "![googlenetModel1Evaluate.png](imgs\\googlenetModel1Evaluate.png)\n",
    "\n",
    "As seen in the screenshot, the run time is around 5 ms to 5.5 ms, and the model accuacy is 73.7704918033%, only slightly off the target of 75%. Based on previous deductions, a second run with more epoch should be able to promote the accuracy above requirement.\n",
    "\n",
    "The model is saved at model\\20180218-223420-306b_epoch_10.0\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second run\n",
    "\n",
    "* Epoch is increased to 15\n",
    "![googLeNet_model_2](imgs\\googleNetModel2.png)\n",
    "\n",
    "This time, the overall trend is almost the same but do notice that the accuracy increases at a lower rate before epoch 2. After 3 epochs, the accuracy reaches over 99%, and after 6 epochs it reaches 100% again.\n",
    "\n",
    "![googlenetModel2Evaluate.png](imgs\\googlenetModel2Evaluate.png)\n",
    "\n",
    "The run time is actually increased a bit to around 5.5 ms, and the model accuacy is 75.4098360656%, successfully achived the target of 75%. \n",
    "\n",
    "The model is saved at model\\20180218-230100-1f9c_epoch_15.0\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Overall this network performs very good, achiving requirments in only two runs with 15 epochs. The reasons, as discussed above, could be due to the comparatively large data set, standardized images, unified backgrounds and environments, and simple objects. And this gives an inspiration on how to organize the images when collecting customized data set for Part B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Classification Network For Cats\n",
    "\n",
    "### Introduction: \n",
    "\n",
    "For this part in the inference project, a classification network to classify cats is proposed. More specifically:\n",
    "\n",
    "There are two cats in my house, with distinctive features.\n",
    "\n",
    "![cats](imgs\\cats.jpg)\n",
    "\n",
    "As you can see from the picture, one of them is a shorthair with black, gray and white color, and is a male named Cucumber. The other is a shorthair with orange color, and is a female named Ginger. This essentially creates 3 categories for the classification network: Cucumber, Ginger, and other cats or animals that are neither of them.\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "* Data set is collected from my own photo collection of the cats, and the internet. <br><br>\n",
    "\n",
    "* A tatal of 2785 images are collected. Among them, a total of 1349 ges are of Cucumber, and a total of 501 images are of Ginger. These images all comes from the photo collection I took for them and are of different angles, poses, and are taken at different times and life stages. The rest are all images of other cats or animals that I randomly saved from the internet. <br><br>\n",
    "\n",
    "* The selection of the photos are intentional so that no photos are included with the presence of both of the cats in frame at the same time. And also the photos are excluded when they include faces of people or the cats are not in the center of the frame. <br><br>\n",
    "\n",
    "* There are repeatitions in the photo collection that may only differ slightly from each other, due to the nature of the high speed continuous capturing in order to get a good picture of the cats. Since they are used for training, however, which means that they will be used in a bunch anyway, a seperation and selection among them are not performed. <br><br>\n",
    "\n",
    "* Normalize the pictures\n",
    "\n",
    "    As observed in Part A, a uniformed, well organized data set can help improve the training process. Thus before importing into DIGITS, besides grouping into labeled folder, a series of normalization procedures are also performed on the images:\n",
    "\n",
    "    1. Batch rename <br>\n",
    "    All the images are named as \"label\" + \"\\_x\" where the x is a sequential number, like \"cucumber_1\", \"cucumber_2\", etc. <br><br>\n",
    "\n",
    "    2. Batch crop and resize <br>\n",
    "    All the images are cropped around the corner and resized to a 256 x 256 square image.  <br><br>\n",
    "\n",
    "    3. Batch convert to .jpg file <br>\n",
    "    All the images are converted to the .jpg format.  <br><br>\n",
    "\n",
    "The data set is then uploaded to the DIGITS server under the /data folder. \n",
    "![createDataSetMyCats](imgs\\createDataSetMyCats.png)\n",
    "\n",
    "* Observe the data set\n",
    "\n",
    "It can be seen here that this data set has 3 categories: cucumber, ginger, and others. After the split for training set and validation set, the numbers of images in each set are:\n",
    "\n",
    "* training set:\n",
    "    * cucumber: 1012\n",
    "    * others: 701\n",
    "    * ginger: 376\n",
    "* validation set:\n",
    "    * cucumber: 337\n",
    "    * others: 234\n",
    "    * ginger: 125\n",
    "    \n",
    "So in total this set has 2785 images, and they are all color images sized 256 x 256. This also confirms that all the images are uploaded and processed correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background / Formulation:\n",
    "\n",
    "* Choose the network\n",
    "\n",
    "Since the data set is normalized like the one used in Part A, it is reasonable to follow the same network selection. So googLeNet is chosen as the target network, and the epoch number is set to 10 just for a quick test run. Eeverything else was kept default. The model was named googLeNet_cats_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First run\n",
    "\n",
    "* Epoch is set to 10\n",
    "![googleNetCatsModel1](imgs\\googleNetCatsModel1.png) \n",
    "    \n",
    "Unlike in Part A, this time it took 6 epochs for the network to gain a mediocre accuracy, and even after the training is done, the accuracy is only 74.43%. A comparison between the data sets used in the two parts reveals several possible causes:\n",
    "\n",
    "  1. Comparing with the data set used in Part A which contains over 10000 images, this one with less than 3000 images is very small. This lack of abundant training materials is one of the reasons why the accuracy is way lower. <br><br>\n",
    "  \n",
    "  2. The images, being taken from a daily photo collection, have various background, lighting, and environments. This creats extra burdens on the network and contributes largely to the low accuracy. <br><br>\n",
    "  \n",
    "  3. The objects are much more complex. Comparing with bottles and boxes that are mostly regularly shaped, the objects here are cats, who are well known for their ability to reshape themselves into many different forms. This variation in forms creats another layer of difficulty for the network and thus further lowers the accuracy. <br><br>  \n",
    "  \n",
    "  4. Comparing to the static objects like bottles and boxes, cats are alive and growing. The second data set contains photos of the cats that span from when they are kittens until they grow into adults, during which time their size, shape and many other features changed drastically. This essentially raised the level of difficulty even further. <br><br>\n",
    "    \n",
    "#### Evaluation\n",
    "\n",
    "Since there's no evaluation script for the customized data set, a manual inspection is performed.\n",
    "\n",
    "* Single image classification <br>\n",
    "\n",
    "  Here are two classification tests performed on Cucumber's images.\n",
    "\n",
    "![testCatsCucumber1](imgs\\testCatsCucumber1.png) <br><br>\n",
    "![testCatsCucumber2](imgs\\testCatsCucumber2.png) <br><br>\n",
    "\n",
    "The network classified them correctly with an accuracy of about 77%, which is higher than the overall accuracy. That means there must be some other images that the network performed poorly on. \n",
    "\n",
    "* Multiple image classification <br>\n",
    "\n",
    "  Use the images for Cucumber again, a multiple image classification test is performed and here is the result:\n",
    "\n",
    "![multipleTestCucumber1](imgs\\multipleTestCucumber1.png) <br><br>\n",
    "\n",
    "All images were classified correctly, but the accuracy are not all that high. More specifically, the image cucumber_4.jpg only scored an accuracy of 51.9%. On that image, the second possible label, others, comes very close at 44.58%. Perform a single image classification on that image:\n",
    "\n",
    "![difficultCucumber](imgs\\difficultCucumber.png) <br><br>\n",
    "\n",
    "It's obvious that this image is hard to classify. There are a lot of other irrelevant objects in it, and the distinction between the cat and the background is not very clear.\n",
    "\n",
    "It is very clear that the network needs more training.\n",
    "\n",
    "The model is saved at model\\catsModel\\20180218-235336-45fe_epoch_10.0\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second run\n",
    "\n",
    "* Epoch is increased to 30\n",
    "\n",
    "![googleNetCatsModel2](imgs\\googleNetCatsModel2.png) <br><br>\n",
    "    \n",
    "This time, the accuracy was increased to 82.67%.\n",
    "    \n",
    "#### Evaluation\n",
    "\n",
    "* Multiple image classification <br>\n",
    "\n",
    "  Use the images for Cucumber.\n",
    "\n",
    "![multipleTestCucumber2](imgs\\multipleTestCucumber2.png) <br><br>\n",
    "\n",
    "All images were classified correctly, and the accuracy were essentially promoted, except cucumber_9.jpg, which actually drops from 62.97% to 43.76%. Perform a single image classification on that image:\n",
    "\n",
    "![difficultCucumber2](imgs\\difficultCucumber2.png) <br><br>\n",
    "\n",
    "It's not so obvious why this image is hard to classify. A speculation would be that there is a wide area of white in the background (the plastic bag) that might have confused the network when mixed with Cucumber's white fur.\n",
    "\n",
    "A multiple image classification test was also done on Ginger's images:\n",
    "![multipleTestGinger1](imgs\\multipleTestGinger1.png) <br><br>\n",
    "\n",
    "Here are several notable observations: <br><br>\n",
    "  1. The network can identify Cucumber, but failed to label all of Ginger's images correctly. Could it be the difference of their fur color? <br><br>\n",
    "  2. Though the network couldn't get all correct on Ginger's images, on the correct ones however, its accuracy was very high (80% ~ 90%); on the contrary, all the accuracy on Cucumber's images are mediocre (just over half for most of the time) <br><br>\n",
    "  \n",
    "Note that 3 images are incorrectly labeled. Single image classification tests are performed on each one of them to further examine the reason.\n",
    "\n",
    "* Single image classification\n",
    "\n",
    "  ![difficultGinger](imgs\\difficultGinger.png)\n",
    "  ![difficultGinger2](imgs\\difficultGinger2.png)\n",
    "  \n",
    "The above two images that were classified as Others. Just by looking, it's very hard to tell why these images were misclassified. But do notice that they persent a similar pattern: there's a large white, furry area in the image surrounding the cat. A reasonable speculation would be that in the Others category there is a similar looking cat that has orange and white fur, and the network was confused by the carpet or pad, thinking they are parts of the cat. \n",
    "\n",
    "![difficultGinger3](imgs\\difficultGinger3.png)\n",
    "\n",
    "The image above was classified as Cucumber instead of Ginger. One possible reason could be that from this angle, most of the facial features are lost, and the white fur on her chest is confusing because Cucumber also has white fur on his chest. \n",
    "\n",
    "![testCatsGinger1.png](imgs\\testCatsGinger1.png)\n",
    "\n",
    "This is an image that was correctly classified as Ginger. As stated before, for the correctly classified images of Ginger, the network can achieve a relatively high accuracy. \n",
    "\n",
    "The model is saved at model\\catsModel\\20180219-010041-24ee_epoch_30.0\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "One major flaw when designing the category is that images of other animals, mostly cats, are used as the third category instead of a \"nothing\" category. Comparing to an empty background, this adds unnecessary complexity to the difficulty for the network, since there might be another cat in the Other category that resembles similar features with Cucumber or Ginger. The evaluation process above also indicates this possibility.\n",
    "\n",
    "Regarding this specific project, accuracy should weight over inference time since it was to be integrated into a monitor network, under which circumstance the speed of the inference is less important since there's no instant reaction required. \n",
    "\n",
    "\n",
    "### Conclusion / Future Work\n",
    "\n",
    "Overall, the network has achieved the goal. Still, there are many speculations that require confirmation. Should more time be granted, the network can be improved even further. Several improvments include:\n",
    "\n",
    "  1. Capture more images in the same environment\n",
    "  2. Use empty background as the third category\n",
    "  3. Train multiple runs, and eliminate images that are not suitable for the task\n",
    "  3. Try other networks, and tweak the parameters\n",
    "\n",
    "* Possible usage <br>\n",
    "  As stated above, if this network is part of a monitor network, then the accuracy should be emphasized over inference time; on the other hand, if the project requires instant reaction, for instance, a \"house defense system\" against rabbits and squirrels, then the inference time should weight more than accuracy, since in that setup, the classification network can be trained to only deal with as few as two categories: friend, or enemy.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
